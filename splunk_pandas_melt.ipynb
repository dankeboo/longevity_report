{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "import pandas as pd\n",
    "from splunksdkexamples.search import main as search\n",
    "\n",
    "def get_df_melt(d):\n",
    "    argv=[\n",
    "        d[\"spl\"],\n",
    "        '--output_mode=csv',\n",
    "    ]\n",
    "    print(f'processing scope: {d[\"scope\"]}')\n",
    "    try:\n",
    "        results = search(argv)\n",
    "        df = pd.read_csv(\n",
    "            io.StringIO(results.read().decode('utf-8')),\n",
    "            encoding='utf8',\n",
    "            sep=',',\n",
    "            low_memory=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f'something went wrong with scope: {d[\"scope\"]} Exception: {e} was caught!!')\n",
    "        return None\n",
    "    else:\n",
    "        df1 = df.melt(id_vars=['time', 'host', 'obj'])\n",
    "        return df1.assign(scope=[d[\"scope\"],] * len(df1.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_spl_list(span, optcmd):\n",
    "\n",
    "    # PerProcess data is recorded every 10 seconds\n",
    "    # Multipler to correct for aggregate (sum) span period\n",
    "    switcher={\n",
    "        \"1d\" : \"*10/60/60/24\",\n",
    "        \"1h\" : \"*10/60/60\",\n",
    "        \"2h\" : \"*10/60/60/2\",\n",
    "        \"3h\" : \"*10/60/60/3\",\n",
    "        \"4h\" : \"*10/60/60/4\",\n",
    "        \"6h\" : \"*10/60/60/6\",\n",
    "        \"12h\" : \"*10/60/60/12\",\n",
    "        \"10s\" : \"*10/10\",\n",
    "        \"30s\" : \"*10/10/3\",\n",
    "        \"1m\" : \"*10/60\",\n",
    "        \"5m\" : \"*10/60/5\",\n",
    "        \"10m\" : \"*10/60/10\",\n",
    "        \"30m\" : \"*10/60/30\",\n",
    "    }\n",
    "\n",
    "    # to get daily volume from span sum\n",
    "    switcher2={\n",
    "        \"1d\" : \"*1\",\n",
    "        \"1h\" : \"*24\",\n",
    "        \"2h\" : \"*24/2\",\n",
    "        \"3h\" : \"*24/3\",\n",
    "        \"4h\" : \"*24/4\",\n",
    "        \"6h\" : \"*24/6\",\n",
    "        \"12h\" : \"*24/12\",\n",
    "        \"10s\" : \"*60*60*24/10\",\n",
    "        \"30s\" : \"*60*60*24/30\",\n",
    "        \"1m\" : \"*60*24\",\n",
    "        \"5m\" : \"*60*24/5\",\n",
    "        \"10m\" : \"*60*24/10\",\n",
    "        \"30m\" : \"*60*24/30\",\n",
    "    }\n",
    "\n",
    "    # to get hourly volume from span sum\n",
    "    switcher3={\n",
    "        \"1d\" : \"/24\",\n",
    "        \"1h\" : \"*1\",\n",
    "        \"2h\" : \"/2\",\n",
    "        \"3h\" : \"/3\",\n",
    "        \"4h\" : \"/4\",\n",
    "        \"6h\" : \"/6\",\n",
    "        \"12h\" : \"/12\",\n",
    "        \"10s\" : \"*6*60\",\n",
    "        \"30s\" : \"*2*60\",\n",
    "        \"1m\" : \"*60\",\n",
    "        \"5m\" : \"*60/5\",\n",
    "        \"10m\" : \"*60/10\",\n",
    "        \"30m\" : \"*60/30\",\n",
    "    }\n",
    "\n",
    "    proc_mp=switcher.get(span, \"Invalid span\")\n",
    "    proc_mp2=switcher2.get(span, \"Invalid span\")\n",
    "    proc_mp3=switcher3.get(span, \"Invalid span\")\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"scope\" : \"01 system\",\n",
    "            \"spl\": f\"\"\"\n",
    "            search index=_introspection {optcmd} sourcetype=splunk_resource_usage component=Hostwide\n",
    "            | bin _time span={span}\n",
    "            | rename data.* as * \n",
    "            | rename component as obj\n",
    "            | eval cpu=cpu_system_pct+cpu_user_pct \n",
    "            | rename _time as time\n",
    "            | stats limit=0 avg(cpu) as avg_cpu%, avg(mem_used) as avg_mem_MiB by time host obj\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"015 ingestion\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=\"_internal\" {optcmd} source=\"*/metrics.log*\"  group=per_sourcetype_thruput \n",
    "            [| rest /services/server/info |where \"indexer\" in(server_roles)| table host | sort host]\n",
    "            | eval gb=kb/1024/1024{proc_mp2}\n",
    "            | bin _time span={span}\n",
    "            | eval obj=\"ingestion\"\n",
    "            | rename _time as time\n",
    "            | eval host=\"clusterwide\"\n",
    "            | stats sum(gb) as GB_per_Day by time host obj\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"0151 ingestion\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=\"_internal\" {optcmd} source=\"*/metrics.log*\"  group=per_sourcetype_thruput \n",
    "            [| rest /services/server/info |where \"indexer\" in(server_roles)| table host | sort host]\n",
    "            | eval gb=kb/1024/1024{proc_mp2}\n",
    "            | bin _time span={span}\n",
    "            | eval obj=\"ingestion\"\n",
    "            | rename _time as time\n",
    "            | stats sum(gb) as GB_per_Day by time host obj\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"016 adhoc search\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_audit {optcmd} \n",
    "            info=completed search_id!=*rsa_* search_id!=*subsearch* is_realtime=0 savedsearch_name=\"\"\n",
    "            | bin _time span={span}\n",
    "            | eval obj=\"adhoc search\"\n",
    "            |rename _time as time\n",
    "            | eval host=\"clusterwide\"\n",
    "            | stats count as hourly_search_count, avg(total_run_time) as avg_search_time by time host obj\n",
    "            | eval hourly_search_count=hourly_search_count{proc_mp3}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"0161 adhoc search\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_audit {optcmd} \n",
    "            info=completed search_id!=*rsa_* search_id!=*subsearch* is_realtime=0 savedsearch_name=\"\"\n",
    "            | bin _time span={span}\n",
    "            | eval obj=\"adhoc search\"\n",
    "            |rename _time as time\n",
    "            | stats count as hourly_search_count, avg(total_run_time) as avg_search_time by time host obj\n",
    "            | eval hourly_search_count=hourly_search_count{proc_mp3}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"017 DMA search\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_audit {optcmd} \n",
    "            info=completed search_id!=*rsa_scheduler_* search_id!=*subsearch* is_realtime=0 savedsearch_name=*_ACCELERATE_*\n",
    "            | bin _time span={span}\n",
    "            | eval obj=\"DMA search\"\n",
    "            | rename _time as time\n",
    "            | eval host=\"clusterwide\"\n",
    "            | stats count as hourly_search_count, avg(total_run_time) as avg_search_time by time host obj\n",
    "            | eval hourly_search_count=hourly_search_count{proc_mp3}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"0171 DMA search\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_audit {optcmd} \n",
    "            info=completed search_id!=*rsa_scheduler_* search_id!=*subsearch* is_realtime=0 savedsearch_name=*_ACCELERATE_*\n",
    "            | bin _time span={span}\n",
    "            | eval obj=\"DMA search\"\n",
    "            | rename _time as time\n",
    "            | stats count as hourly_search_count, avg(total_run_time) as avg_search_time by time host obj\n",
    "            | eval hourly_search_count=hourly_search_count{proc_mp3}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"018 ES correlation search\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_audit {optcmd} \n",
    "            info=completed search_id!=*rsa_scheduler_* search_id!=*subsearch* is_realtime=0 savedsearch_name=\"*- Rule\"\n",
    "            | bin _time span={span}\n",
    "            | eval obj=\"ES correlation search\"\n",
    "            | rename _time as time\n",
    "            | eval host=\"clusterwide\"\n",
    "            | stats count as hourly_search_count, avg(total_run_time) as avg_search_time by time host obj\n",
    "            | eval hourly_search_count=hourly_search_count{proc_mp3}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"0181 ES correlation search\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_audit {optcmd} \n",
    "            info=completed search_id!=*rsa_scheduler_* search_id!=*subsearch* is_realtime=0 savedsearch_name=\"*- Rule\"\n",
    "            | bin _time span={span}\n",
    "            | eval obj=\"ES correlation search\"\n",
    "            | rename _time as time\n",
    "            | stats count as hourly_search_count, avg(total_run_time) as avg_search_time by time host obj\n",
    "            | eval hourly_search_count=hourly_search_count{proc_mp3}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"019 ES saved search\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_audit {optcmd} \n",
    "            info=completed search_id!=*rsa_scheduler_* search_id!=*subsearch* is_realtime=0 savedsearch_name!=\"*- Rule\" savedsearch_name!=\"_ACCELERATE_*\" savedsearch_name!=\"\"\n",
    "            | bin _time span={span}\n",
    "            | eval obj=\"ES saved search\"\n",
    "            | rename _time as time\n",
    "            | eval host=\"clusterwide\"\n",
    "            | stats count as hourly_search_count, avg(total_run_time) as avg_search_time by time host obj\n",
    "            | eval hourly_search_count=hourly_search_count{proc_mp3}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"0191 ES saved search\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_audit {optcmd} \n",
    "            info=completed search_id!=*rsa_scheduler_* search_id!=*subsearch* is_realtime=0 savedsearch_name!=\"*- Rule\" savedsearch_name!=\"_ACCELERATE_*\" savedsearch_name!=\"\"\n",
    "            | bin _time span={span}\n",
    "            | eval obj=\"ES saved search\"\n",
    "            | rename _time as time\n",
    "            | stats count as hourly_search_count, avg(total_run_time) as avg_search_time by time host obj\n",
    "            | eval hourly_search_count=hourly_search_count{proc_mp3}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"02 proc\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_introspection {optcmd} sourcetype=splunk_resource_usage component=PerProcess\n",
    "            | rename data.* as * \n",
    "            | rename process as obj\n",
    "            | eval pct_cpu = pct_cpu{proc_mp}\n",
    "            | eval mem_used = mem_used{proc_mp}\n",
    "            | eval fd_used = fd_used{proc_mp}\n",
    "            | eval t_count = t_count{proc_mp}\n",
    "            | bin _time span={span}\n",
    "            | rename _time as time\n",
    "            | stats limit=0 sum(pct_cpu) as avg_cpu%, sum(mem_used) as avg_mem_MiB\n",
    "              sum(fd_used) as avg_fd_usage, sum(t_count) as avg_threads \n",
    "             by time host obj\n",
    "            \"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"scope\": \"03 proc_class\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_introspection {optcmd} sourcetype=splunk_resource_usage component=PerProcess \n",
    "            (host=sh1 OR host=idx1 OR host=master1)\n",
    "            | eval process = 'data.process' | eval args = 'data.args' | eval pid = 'data.pid' \n",
    "            | eval ppid = 'data.ppid' | eval elapsed = 'data.elapsed' | eval mem_used = 'data.mem_used' \n",
    "            | eval mem = 'data.mem' | eval pct_memory = 'data.pct_memory' | eval pct_cpu = 'data.pct_cpu' \n",
    "            | eval fd = 'data.fd_used' \n",
    "            | eval t_count = 'data.t_count' \n",
    "            | eval sid = 'data.search_props.sid' | eval app = 'data.search_props.app' \n",
    "            | eval label = 'data.search_props.label' | eval type = 'data.search_props.type' \n",
    "            | eval mode = 'data.search_props.mode' | eval user = 'data.search_props.user' \n",
    "            | eval role = 'data.search_props.role' \n",
    "            | eval label = if(isnotnull('data.search_props.label'), 'data.search_props.label', \"\") \n",
    "            | eval provenance = if(isnotnull('data.search_props.provenance'), 'data.search_props.provenance', \"unknown\") \n",
    "            | eval search_head = case(isnotnull('data.search_props.search_head') \n",
    "            AND 'data.search_props.role' == \"peer\", 'data.search_props.search_head', isnull('data.search_props.search_head') \n",
    "            AND 'data.search_props.role' == \"head\", \"_self\", isnull('data.search_props.search_head') \n",
    "            AND 'data.search_props.role' == \"peer\", \"_unknown\") \n",
    "            | eval workload_pool = if(isnotnull('data.workload_pool'), 'data.workload_pool', \"UNDEFINED\") \n",
    "            | eval process_class = case( process==\"splunk-optimize\",\"index service\", process==\"sh\" \n",
    "            OR process==\"ksh\" OR process==\"bash\" OR like(process,\"python%\") \n",
    "            OR process==\"powershell\",\"scripted input\", process==\"mongod\", \"KVStore\") \n",
    "            | eval process_class = case( process==\"splunkd\" \n",
    "            AND (like(args,\"-p %start%\") OR like(args,\"service\") \n",
    "            OR like(args,\"%_internal_launch_under_systemd%\")),\"splunkd server\", process==\"splunkd\" \n",
    "            AND isnotnull(sid) AND type==\"datamodel acceleration\",\"datamodel acceleration\", process==\"splunkd\" \n",
    "            AND isnotnull(sid) AND type==\"scheduled\",\"scheduler searches\", process==\"splunkd\" \n",
    "            AND isnotnull(sid) AND type==\"ad-hoc\",\"ad-hoc searches\", process==\"splunkd\" \n",
    "            AND isnotnull(sid) AND type==\"summary index\",\"summary index\", process==\"splunkd\" \n",
    "            AND (like(args,\"fsck%\") OR like(args,\"recover-metadata%\") \n",
    "            OR like(args,\"cluster_thing\")),\"index service\", process==\"splunkd\" \n",
    "            AND args==\"instrument-resource-usage\", \"scripted input\", (like(process,\"python%\") \n",
    "            AND like(args,\"%/appserver/mrsparkle/root.py%\")) \n",
    "            OR like(process,\"splunkweb\"),\"Splunk Web\", isnotnull(process_class), process_class) \n",
    "            | eval process_class = if(isnull(process_class),\"other\",process_class) \n",
    "            | bin _time span=10s \n",
    "            | stats \n",
    "            latest(pct_cpu) AS resource_usage_dedup \n",
    "            latest(mem_used) AS mem_usage_dedup \n",
    "            latest(fd) AS fd_usage_dedup\n",
    "            latest(t_count) AS t_count_usage_dedup\n",
    "            latest(process_class) AS process_class by pid, _time, host \n",
    "            | stats sum(resource_usage_dedup) AS resource_usage \n",
    "            sum(mem_usage_dedup) AS mem_usage \n",
    "            sum(fd_usage_dedup) AS fd_usage \n",
    "            sum(t_count_usage_dedup) AS t_usage  \n",
    "            by _time, process_class, host \n",
    "            | bin _time span={span} \n",
    "            | rename process_class as obj\n",
    "            | rename _time as time\n",
    "            | stats \n",
    "            avg(resource_usage) AS \"avg_cpu%\" \n",
    "            avg(mem_usage) AS \"avg_mem_MiB\" \n",
    "            avg(fd_usage) AS \"avg_fd_usage\"\n",
    "            avg(t_usage) AS \"avg_threads\"\n",
    "            by time, host, obj\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"04 crash\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_internal {optcmd} source=*crash* \n",
    "            | rex field=_raw \"Crashing thread: (?<crash_thread>\\w+)\" \n",
    "            | bin _time span={span} \n",
    "            | rename crash_thread as obj\n",
    "            | rename _time as time\n",
    "            | stats count by time host obj\n",
    "            \"\"\"\n",
    "            \n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"05 dma_search\",\n",
    "            \"spl\" :     f\"\"\"\n",
    "            search index=_internal {optcmd} sourcetype=scheduler \n",
    "            source=*/scheduler*.log search_type=datamodel_acceleration savedsearch_name=* status!=*_remote* \n",
    "            |rename savedsearch_name as obj\n",
    "            |bin _time span={span}\n",
    "            | rename _time as time\n",
    "            | stats distinct_count(scheduled_time) as total_count, \n",
    "            count(eval(status=\"success\")) as success_count by time host obj \n",
    "            | eval failed_count=total_count-success_count \n",
    "            | eval failed_ratio=failed_count/total_count*100\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"scope\" : \"06 buckets_tx\",\n",
    "            \"spl\" : f\"\"\"\n",
    "            search index=_audit {optcmd} (action=remote_bucket_download OR action=local_bucket_upload \n",
    "            OR action=local_bucket_evict OR action=remote_bucket_remove) info=completed cache_id=\"*bid|*\" \n",
    "            | rename action as obj\n",
    "            |bin _time span={span}\n",
    "            | rename _time as time\n",
    "            | stats sum(kb) as throughput_KB by time host obj\n",
    "            \"\"\"\n",
    "        }\n",
    "  \n",
    "    ]\n",
    "\n",
    "\n",
    "def get_results(span='1d', optcmd=''):\n",
    "    return pd.concat(\n",
    "        map(get_df_melt, do_spl_list(span, optcmd))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing scope: 01 system\n",
      "processing scope: 015 ingestion\n",
      "processing scope: 0151 ingestion\n",
      "processing scope: 016 adhoc search\n",
      "processing scope: 0161 adhoc search\n",
      "processing scope: 017 DMA search\n",
      "processing scope: 0171 DMA search\n",
      "processing scope: 018 ES correlation search\n",
      "processing scope: 0181 ES correlation search\n",
      "processing scope: 019 ES saved search\n",
      "processing scope: 0191 ES saved search\n",
      "processing scope: 02 proc\n",
      "processing scope: 03 proc_class\n",
      "processing scope: 04 crash\n",
      "something went wrong with scope: 04 crash Exception: No columns to parse from file was caught!!\n",
      "processing scope: 05 dma_search\n",
      "processing scope: 06 buckets_tx\n"
     ]
    }
   ],
   "source": [
    "df=get_results('1h', 'earliest=-5d')\n",
    "outfilename = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yhirasawa/Desktop/longevity_report/vbc-venv/lib/python3.9/site-packages/rpy2/robjects/pandas2ri.py:56: UserWarning: DataFrame contains duplicated elements in the index, which will lead to loss of the row names in the resulting data.frame\n",
      "  warnings.warn('DataFrame contains duplicated elements in the index, '\n"
     ]
    }
   ],
   "source": [
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.ipython import html\n",
    "from functools import partial\n",
    "\n",
    "html.init_printing()\n",
    "html.html_rdataframe=partial(html.html_rdataframe, table_class=\"docutils\")\n",
    "\n",
    "with ro.default_converter + pandas2ri.converter:\n",
    "    r_df = ro.conversion.get_conversion().py2rpy(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext rpy2.ipython\n",
    "%Rpush r_df\n",
    "%Rpush outfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`geom_line()`: Each group consists of only one observation.\n",
      "â„¹ Do you need to adjust the group aesthetic?\n",
      "quartz_off_screen \n",
      "                2 \n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library('ggplot2')\n",
    "library('plyr')\n",
    "\n",
    "r_df$time <- as.POSIXct(r_df$time, origin=\"1970-01-01\")\n",
    "\n",
    "write.csv(r_df, paste0('/Users/yhirasawa/Desktop/',outfilename,'.csv'))\n",
    "pdf(paste0('/Users/yhirasawa/Desktop/',outfilename,'.pdf'))\n",
    "d_ply(\n",
    "    r_df,\n",
    "    .(scope,obj),\n",
    "    function (d){\n",
    "        p <- ggplot(d) + aes(time, value, color=host, group=host) +\n",
    "        #geom_point() +\n",
    "        geom_line() +\n",
    "        facet_wrap(~variable, ncol=1, scale='free_y') +\n",
    "        theme(legend.position = \"none\", legend.text=element_text(size=3)) + \n",
    "        ggtitle(paste(d$scope[1],\"::\", d$obj[1]))\n",
    "        print(p)\n",
    "    }\n",
    ")\n",
    "dev.off()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vbc-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46ac4b90797e56e6e432fabd5e5ab286f2c50dae596b29473b191b3fbb76096c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
